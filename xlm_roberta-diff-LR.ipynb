{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"xlm_roberta-diffLR-with-multilingual-training-data.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"_cell_guid":"c4ad26dd-8d2a-43e9-a3ea-0a4342a16e52","_uuid":"07d7bc95-0377-4db3-aa05-2bd6cf989e78","id":"0hhy7fHVTg3p","colab_type":"code","colab":{},"outputId":"0620bf80-c086-49d9-c0f9-6e98f31f51ca"},"source":["MAX_LEN = 192 \n","DROPOUT = 0.5   # use aggressive dropout\n","BATCH_SIZE = 16 # per TPU core\n","TOTAL_STEPS_STAGE1 = 2000\n","VALIDATE_EVERY_STAGE1 = 200\n","\n","TOTAL_STEPS_STAGE2 = 200\n","VALIDATE_EVERY_STAGE2 = 10\n","\n","### Different learning rate for transformer and head ###\n","LR_TRANSFORMER = 5e-6\n","LR_HEAD = 1e-3\n","\n","PRETRAINED_TOKENIZER=  'jplu/tf-xlm-roberta-large'\n","PRETRAINED_MODEL = '/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large'\n","D = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'\n","D_TRANS = '/kaggle/input/jigsaw-train-multilingual-coments-google-api/'\n","\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","print(tf.__version__)\n","from tensorflow.keras.layers import Dense, Input, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","import transformers\n","from transformers import TFRobertaModel, AutoTokenizer, AutoConfig\n","import logging\n","import random\n","# no extensive logging \n","logging.getLogger().setLevel(logging.NOTSET)\n","\n","AUTO = tf.data.experimental.AUTOTUNE"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.2.0\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"_cell_guid":"86a303dd-5d1d-4d3f-bf28-4a02b6816e9b","_uuid":"5a5fd45e-8f97-44d8-95f6-29cc8563dc3d","id":"1bvXQykGTg3v","colab_type":"code","colab":{},"outputId":"8f0cc2ea-d8bf-49d3-9e4c-5ba5b08e9e59"},"source":["def connect_to_TPU():\n","    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n","    try:\n","        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n","        # set: this is always the case on Kaggle.\n","        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","        print('Running on TPU ', tpu.master())\n","    except ValueError:\n","        tpu = None\n","\n","    if tpu:\n","        tf.config.experimental_connect_to_cluster(tpu)\n","        tf.tpu.experimental.initialize_tpu_system(tpu)\n","        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","    else:\n","        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n","        strategy = tf.distribute.get_strategy()\n","\n","    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n","\n","    return tpu, strategy, global_batch_size\n","\n","\n","tpu, strategy, global_batch_size = connect_to_TPU()\n","print(\"REPLICAS: \", strategy.num_replicas_in_sync)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Running on TPU  grpc://10.0.0.2:8470\n","REPLICAS:  8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_cell_guid":"1cba2ff3-145e-4187-a6a4-60d28adf5141","_uuid":"80b620f4-85cd-4436-b868-acdcea7bf2b6","id":"a03TXi_rTg30","colab_type":"code","colab":{}},"source":["def load_jigsaw_trans(langs=['tr','it','es','ru','fr','pt'], \n","                      columns=['comment_text', 'toxic']):\n","    train_6langs=[]\n","    for i in range(len(langs)):\n","        fn = D_TRANS+'jigsaw-toxic-comment-train-google-%s-cleaned.csv'%langs[i]\n","        train_6langs.append(downsample(pd.read_csv(fn)[columns]))\n","        \n","    train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n","    train2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\n","    train2.toxic = train2.toxic.round().astype(int)\n","    df_train2 = pd.concat([train2[['comment_text', 'toxic']].query('toxic==1'),\n","                           train2[['comment_text', 'toxic']].query('toxic==0').sample(n=int(sum(train2.toxic)))])\n","    \n","    train_6langs.append(df_train2)\n","    del train1,train2\n","    return train_6langs\n","\n","def downsample(df):\n","    ds_df= pd.concat([\n","        df.query('toxic==1'),\n","        df.query('toxic==0').sample(n=int(sum(df.toxic)))\n","    ])\n","    return ds_df\n","    \n","\n","train_df = pd.concat(load_jigsaw_trans()) \n","val_df = pd.read_csv(D+'validation.csv')\n","test_df = pd.read_csv(D+'test.csv')\n","sub_df = pd.read_csv(D+'sample_submission.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"900f2d01-b87c-42e7-8cc2-8dcba95ffa54","_uuid":"7d5605a3-df77-4574-9899-89529d0a6e61","id":"2o8sM3ljTg33","colab_type":"code","colab":{},"outputId":"dd70bce4-9bd5-40a0-c891-4521273aaa7e"},"source":["%%time\n","\n","def regular_encode(texts, tokenizer, maxlen=512):\n","    enc_di = tokenizer.batch_encode_plus(\n","        texts, \n","        return_attention_masks=False, \n","        return_token_type_ids=False,\n","        pad_to_max_length=True,\n","        max_length=maxlen\n","    )\n","    \n","    return np.array(enc_di['input_ids'])\n","    \n","\n","tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_TOKENIZER)\n","X_train = regular_encode(train_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n","X_val = regular_encode(val_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n","X_test = regular_encode(test_df.content.values, tokenizer, maxlen=MAX_LEN)\n","\n","y_train = train_df.toxic.values.reshape(-1,1)\n","y_val = val_df.toxic.values.reshape(-1,1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 6min 39s, sys: 1.87 s, total: 6min 41s\n","Wall time: 6min 42s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_cell_guid":"757b2c93-9c07-441d-ba2e-47b8607b3795","_uuid":"8b748024-314d-4256-b67b-0bc71ecac19d","id":"li9thb92Tg36","colab_type":"code","colab":{}},"source":["def create_dist_dataset(X, y=None, training=False):\n","    dataset = tf.data.Dataset.from_tensor_slices(X)\n","\n","    ### Add y if present ###\n","    if y is not None:\n","        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n","        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n","        \n","    ### Repeat if training ###\n","    if training:\n","        dataset = dataset.shuffle(len(X)).repeat()\n","\n","    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n","\n","    ### make it distributed  ###\n","    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n","\n","    return dist_dataset\n","    \n","    \n","train_dist_dataset = create_dist_dataset(X_train, y_train, True)\n","val_dist_dataset   = create_dist_dataset(X_val)\n","test_dist_dataset  = create_dist_dataset(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"3ba0b8c0-3e4c-4afe-8766-844c6b6a8c0f","_uuid":"9f6161c2-a256-4285-9a0e-f23700f4b195","id":"OR1MIYEOTg3_","colab_type":"code","colab":{},"outputId":"34349ec6-c0aa-471f-b309-d2b888e8b450"},"source":["%%time\n","\n","def create_model_and_optimizer():\n","    with strategy.scope():\n","        config = AutoConfig.from_pretrained(PRETRAINED_MODEL)\n","        config.output_hidden_states = True\n","        transformer_layer = TFRobertaModel.from_pretrained(PRETRAINED_MODEL,config=config)                \n","        model = build_model(transformer_layer)\n","        optimizer_transformer = Adam(learning_rate=LR_TRANSFORMER)\n","        optimizer_head = Adam(learning_rate=LR_HEAD)\n","    return model, optimizer_transformer, optimizer_head\n","\n","\n","def build_model(transformer):\n","    inp = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n","    # Huggingface transformers have multiple outputs, embeddings are the first one\n","    # let's slice out the first position, the paper says its not worse than pooling\n","    #x = transformer(inp)[0][:, 0, :]\n","    embedding, pooler_output, hidden_states = transformer(inp)\n","    pooler = tf.reshape(pooler_output,(-1,1,1024))\n","    h12 = tf.reshape(hidden_states[-1][:,0],(-1,1,1024))\n","    h11 = tf.reshape(hidden_states[-1][:,0],(-1,1,1024))\n","    concat_hidden = tf.keras.layers.Concatenate(axis=2)([pooler, h12, h11])\n","    x = tf.keras.layers.GlobalAveragePooling1D()(concat_hidden)\n","    x = Dropout(DROPOUT)(x)\n","    ### note, adding the name to later identify these weights for different LR\n","    out = Dense(1, activation='sigmoid', name='custom_head')(x)\n","    model = Model(inputs=[inp], outputs=[out])\n","    \n","    return model\n","\n","\n","model, optimizer_transformer, optimizer_head = create_model_and_optimizer()\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_word_ids (InputLayer)     [(None, 192)]        0                                            \n","__________________________________________________________________________________________________\n","tf_roberta_model (TFRobertaMode ((None, 192, 1024),  559890432   input_word_ids[0][0]             \n","__________________________________________________________________________________________________\n","tf_op_layer_strided_slice (Tens [(None, 1024)]       0           tf_roberta_model[0][26]          \n","__________________________________________________________________________________________________\n","tf_op_layer_strided_slice_1 (Te [(None, 1024)]       0           tf_roberta_model[0][26]          \n","__________________________________________________________________________________________________\n","tf_op_layer_Reshape (TensorFlow [(None, 1, 1024)]    0           tf_roberta_model[0][1]           \n","__________________________________________________________________________________________________\n","tf_op_layer_Reshape_1 (TensorFl [(None, 1, 1024)]    0           tf_op_layer_strided_slice[0][0]  \n","__________________________________________________________________________________________________\n","tf_op_layer_Reshape_2 (TensorFl [(None, 1, 1024)]    0           tf_op_layer_strided_slice_1[0][0]\n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 1, 3072)      0           tf_op_layer_Reshape[0][0]        \n","                                                                 tf_op_layer_Reshape_1[0][0]      \n","                                                                 tf_op_layer_Reshape_2[0][0]      \n","__________________________________________________________________________________________________\n","global_average_pooling1d (Globa (None, 3072)         0           concatenate[0][0]                \n","__________________________________________________________________________________________________\n","dropout_74 (Dropout)            (None, 3072)         0           global_average_pooling1d[0][0]   \n","__________________________________________________________________________________________________\n","custom_head (Dense)             (None, 1)            3073        dropout_74[0][0]                 \n","==================================================================================================\n","Total params: 559,893,505\n","Trainable params: 559,893,505\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","CPU times: user 31.8 s, sys: 25.3 s, total: 57.1 s\n","Wall time: 58.5 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_cell_guid":"20b20e6a-16c4-475f-92ef-2a7809d21621","_uuid":"50c766ac-68bd-4884-ba54-657f5d385786","id":"vkI5xDMaTg4E","colab_type":"code","colab":{}},"source":["def define_losses_and_metrics():\n","    with strategy.scope():\n","        loss_object = tf.keras.losses.BinaryCrossentropy(\n","            reduction=tf.keras.losses.Reduction.NONE, from_logits=False)\n","\n","        def compute_loss(labels, predictions):\n","            per_example_loss = loss_object(labels, predictions)\n","            loss = tf.nn.compute_average_loss(\n","                per_example_loss, global_batch_size = global_batch_size)\n","            return loss\n","\n","        train_accuracy_metric = tf.keras.metrics.AUC(name='training_AUC')\n","\n","    return compute_loss, train_accuracy_metric\n","\n","\n","def train(train_dist_dataset, val_dist_dataset=None, y_val=None,\n","          total_steps=2000, validate_every=200):\n","    best_weights, history = None, []\n","    step = 0\n","    ### Training lopp ###\n","    for tensor in train_dist_dataset:\n","        distributed_train_step(tensor) \n","        step+=1\n","\n","        if (step % validate_every == 0):   \n","            ### Print train metrics ###  \n","            train_metric = train_accuracy_metric.result().numpy()\n","            print(\"Step %d, train AUC: %.5f\" % (step, train_metric))   \n","            \n","            ### Test loop with exact AUC ###\n","            if val_dist_dataset:\n","                val_metric = roc_auc_score(y_val, predict(val_dist_dataset))\n","                print(\"Step %d,   val AUC: %.5f\" %  (step,val_metric))   \n","                \n","                # save weights if it is the best yet\n","                history.append(val_metric)\n","                if history[-1] == max(history):\n","                    best_weights = model.get_weights()\n","\n","            ### Reset (train) metrics ###\n","            train_accuracy_metric.reset_states()\n","            \n","        if step  == total_steps:\n","            break\n","    \n","    ### Restore best weighths ###\n","    model.set_weights(best_weights)\n","\n","\n","\n","@tf.function\n","def distributed_train_step(data):\n","    strategy.experimental_run_v2(train_step, args=(data,))\n","\n","def train_step(inputs):\n","    features, labels = inputs\n","    \n","    ### get transformer and head separate vars\n","    # get rid of pooler head with None gradients\n","    transformer_trainable_variables = [ v for v in model.trainable_variables \n","                                       if (('pooler' not in v.name)  and \n","                                           ('custom' not in v.name))]\n","    head_trainable_variables = [ v for v in model.trainable_variables \n","                                if 'custom'  in v.name]\n","\n","    # calculate the 2 gradients ( note persistent, and del)\n","    with tf.GradientTape(persistent=True) as tape:\n","        predictions = model(features, training=True)\n","        loss = compute_loss(labels, predictions)\n","    gradients_transformer = tape.gradient(loss, transformer_trainable_variables)\n","    gradients_head = tape.gradient(loss, head_trainable_variables)\n","    del tape\n","        \n","    ### make the 2 gradients steps\n","    optimizer_transformer.apply_gradients(zip(gradients_transformer, \n","                                              transformer_trainable_variables))\n","    optimizer_head.apply_gradients(zip(gradients_head, \n","                                       head_trainable_variables))\n","\n","    train_accuracy_metric.update_state(labels, predictions)\n","\n","\n","\n","def predict(dataset):  \n","    predictions = []\n","    for tensor in dataset:\n","        predictions.append(distributed_prediction_step(tensor))\n","    ### stack replicas and batches\n","    predictions = np.vstack(list(map(np.vstack,predictions)))\n","    return predictions\n","\n","@tf.function\n","def distributed_prediction_step(data):\n","    predictions = strategy.experimental_run_v2(prediction_step, args=(data,))\n","    return strategy.experimental_local_results(predictions)\n","\n","def prediction_step(inputs):\n","    features = inputs  # note datasets used in prediction do not have labels\n","    predictions = model(features, training=False)\n","    return predictions\n","\n","\n","compute_loss, train_accuracy_metric = define_losses_and_metrics()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"d3d0e97f-d7d4-4d3f-8c6e-925669565248","_uuid":"b30a95a7-ae7e-40e8-846b-9d31bfe997ad","id":"MxrVNoJETg4I","colab_type":"code","colab":{},"outputId":"a1f0730a-c1b8-4ad6-aa57-b2e56cd7525c"},"source":["%%time\n","train(train_dist_dataset, val_dist_dataset, y_val,\n","      TOTAL_STEPS_STAGE1, VALIDATE_EVERY_STAGE1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n","  num_elements)\n"],"name":"stderr"},{"output_type":"stream","text":["Step 200, train AUC: 0.84823\n","Step 200,   val AUC: 0.93511\n","Step 400, train AUC: 0.95363\n","Step 400,   val AUC: 0.93679\n","Step 600, train AUC: 0.95802\n","Step 600,   val AUC: 0.94154\n","Step 800, train AUC: 0.96180\n","Step 800,   val AUC: 0.94209\n","Step 1000, train AUC: 0.96359\n","Step 1000,   val AUC: 0.94165\n","Step 1200, train AUC: 0.96421\n","Step 1200,   val AUC: 0.94294\n","Step 1400, train AUC: 0.96633\n","Step 1400,   val AUC: 0.94309\n","Step 1600, train AUC: 0.96609\n","Step 1600,   val AUC: 0.94547\n","Step 1800, train AUC: 0.96788\n","Step 1800,   val AUC: 0.94201\n","Step 2000, train AUC: 0.96673\n","Step 2000,   val AUC: 0.94264\n","CPU times: user 3min 25s, sys: 56 s, total: 4min 21s\n","Wall time: 17min 48s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_cell_guid":"1bfbb20f-409d-487f-bdf0-fca60927a868","_uuid":"d5b8523a-69d6-41f0-971b-8a2bfcb9e908","id":"QJUMJRFATg4L","colab_type":"code","colab":{},"outputId":"b4b0d58d-fa6f-4b20-d9ad-cca6af1c4752"},"source":["%%time\n","\n","# decrease LR for second stage in the head\n","optimizer_head.learning_rate.assign(5e-5)\n","\n","# split validation data into train test\n","X_train, X_val, y_train, y_val = train_test_split(X_val, y_val, test_size = 0.1)\n","\n","# make a datasets\n","train_dist_dataset = create_dist_dataset(X_train, y_train, training=True)\n","val_dist_dataset = create_dist_dataset(X_val, y_val)\n","\n","# train again\n","train(train_dist_dataset, val_dist_dataset, y_val,\n","      total_steps = TOTAL_STEPS_STAGE2, \n","      validate_every = VALIDATE_EVERY_STAGE2)  # not validating but printing now"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Step 10, train AUC: 0.90205\n","Step 10,   val AUC: 0.94612\n","Step 20, train AUC: 0.93556\n","Step 20,   val AUC: 0.95038\n","Step 30, train AUC: 0.93457\n","Step 30,   val AUC: 0.95192\n","Step 40, train AUC: 0.94269\n","Step 40,   val AUC: 0.95195\n","Step 50, train AUC: 0.94830\n","Step 50,   val AUC: 0.95394\n","Step 60, train AUC: 0.94289\n","Step 60,   val AUC: 0.95425\n","Step 70, train AUC: 0.95567\n","Step 70,   val AUC: 0.95519\n","Step 80, train AUC: 0.95110\n","Step 80,   val AUC: 0.95384\n","Step 90, train AUC: 0.94708\n","Step 90,   val AUC: 0.95588\n","Step 100, train AUC: 0.96031\n","Step 100,   val AUC: 0.95654\n","Step 110, train AUC: 0.95329\n","Step 110,   val AUC: 0.95485\n","Step 120, train AUC: 0.96566\n","Step 120,   val AUC: 0.95479\n","Step 130, train AUC: 0.97294\n","Step 130,   val AUC: 0.95384\n","Step 140, train AUC: 0.95738\n","Step 140,   val AUC: 0.95486\n","Step 150, train AUC: 0.96516\n","Step 150,   val AUC: 0.95561\n","Step 160, train AUC: 0.96447\n","Step 160,   val AUC: 0.95398\n","Step 170, train AUC: 0.95656\n","Step 170,   val AUC: 0.95372\n","Step 180, train AUC: 0.97225\n","Step 180,   val AUC: 0.95540\n","Step 190, train AUC: 0.96506\n","Step 190,   val AUC: 0.95376\n","Step 200, train AUC: 0.97570\n","Step 200,   val AUC: 0.95618\n","CPU times: user 58.1 s, sys: 38.3 s, total: 1min 36s\n","Wall time: 3min 22s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_cell_guid":"96a7be93-0dce-4d00-92c4-d4c4cb2238f8","_uuid":"5cd548e4-5756-4d92-b72c-f71cb72b395d","id":"xlj0YvH9Tg4P","colab_type":"code","colab":{},"outputId":"cccad722-5851-4e46-fb07-acd1cc88b01f"},"source":["%%time\n","sub_df['toxic'] = predict(test_dist_dataset)[:,0]\n","sub_df.to_csv('submission.csv', index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 21.9 s, sys: 4.12 s, total: 26.1 s\n","Wall time: 1min 28s\n"],"name":"stdout"}]}]}